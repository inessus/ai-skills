# PYTORCH-WAVENET-VOCODER

[![Build Status](https://travis-ci.org/kan-bayashi/PytorchWaveNetVocoder.svg?branch=master)](https://travis-ci.org/kan-bayashi/PytorchWaveNetVocoder)

This repository is the wavenet-vocoder implementation with pytorch.

![](https://github.com/kan-bayashi/WaveNetVocoderSamples/blob/master/figure/overview.bmp)

## Key features

- Support kaldi-like recipe, easy to reproduce the results
- Support multi-gpu training / decoding
- Support world features / mel-spectrogram as auxiliary features
- Support recipes of three public databases

    - [CMU Arctic database](http://www.festvox.org/cmu_arctic/): `egs/arctic`
    - [LJ Speech database](https://keithito.com/LJ-Speech-Dataset/): `egs/ljspeech`
    - [M-AILABS speech database](http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/): `egs/m-ailabs-speech`

## Requirements

- python 3.6
- virtualenv
- cuda 8.0
- cndnn 6
- nccl 2.0+ (for the use of multi-gpus)

Recommend to use the GPU with 10GB> memory.

## Setup

```bash
$ git clone https://github.com/kan-bayashi/PytorchWaveNetVocoder.git
$ cd PytorchWaveNetVocoder/tools
$ make
```

## How-to-run

```bash
$ cd egs/arctic/sd
$ ./run.sh
```

See more detail of the recipes in [egs/README.md](egs/README.md).

## Results

This is the subjective evaluation results using `arctic` recipe.
![](https://github.com/kan-bayashi/WaveNetVocoderSamples/blob/master/figure/mos.bmp)

You can listen the samples generated by our models from [here](https://drive.google.com/drive/folders/1zC1WDiMu4SOdc7UeOayoEe_79PdnPBu6?usp=sharing).

- `arctic_raw_16k.wav`: original in arctic database
- `arctic_sd_16k_world.wav`: sd model with world aux feats + noise shaping with world mcep
- `arctic_si-open_16k_world.wav`: si-open model with world aux feats + noise shaping with world mcep
- `arctic_si-close_16k_world.wav`: si-close model with world aux feats + noise shaping with world mcep
- `arctic_si-close_16k_melspc.wav`: si-close model with mel-spectrogram aux feats
- `arctic_si-close_16k_melspc_ns.wav`: si-close model with mel-spectrogram aux feats + noise shaping with stft mcep
- `ljspeech_raw_22.05k.wav`: original in ljspeech database
- `ljspeech_sd_22.05k_world.wav`: sd model with world aux feats + noise shaping with world mcep
- `ljspeech_sd_22.05k_melspc.wav`: sd model with mel-spectrogram aux feats
- `ljspeech_sd_22.05k_melspc_ns.wav`: sd model with mel-spectrogram aux feats + noise shaping with stft mcep
- `m-ailabs_raw_16k.wav`: original in m-ailabs speech database
- `m-ailabs_sd_16k_melspc.wav`: sd model with mel-spectrogram aux feats

## References

Please cite the following articles.

```
@inproceedings{tamamori2017speaker,
  title={Speaker-dependent WaveNet vocoder},
  author={Tamamori, Akira and Hayashi, Tomoki and Kobayashi, Kazuhiro and Takeda, Kazuya and Toda, Tomoki},
  booktitle={Proceedings of Interspeech},
  pages={1118--1122},
  year={2017}
}
@inproceedings{hayashi2017multi,
  title={An Investigation of Multi-Speaker Training for WaveNet Vocoder},
  author={Hayashi, Tomoki and Tamamori, Akira and Kobayashi, Kazuhiro and Takeda, Kazuya and Toda, Tomoki},
  booktitle={Proc. ASRU 2017},
  year={2017}
}
@article{hayashi2018sp,
  title={複数話者WaveNetボコーダに関する調査}.
  author={林知樹 and 小林和弘 and 玉森聡 and 武田一哉 and 戸田智基},
  journal={電子情報通信学会技術研究報告},
  year={2018}
}
```

## Author

Tomoki Hayashi @ Nagoya University  
e-mail:hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp
